<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://tonychenxyz.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://tonychenxyz.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-10-10T18:16:33+00:00</updated><id>https://tonychenxyz.github.io/feed.xml</id><title type="html">Tony Chen</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Understand Policy Gradient by Building Cross Entropy from Scratch</title><link href="https://tonychenxyz.github.io/blog/2023/understand-rl/" rel="alternate" type="text/html" title="Understand Policy Gradient by Building Cross Entropy from Scratch"/><published>2023-06-11T16:40:16+00:00</published><updated>2023-06-11T16:40:16+00:00</updated><id>https://tonychenxyz.github.io/blog/2023/understand-rl</id><content type="html" xml:base="https://tonychenxyz.github.io/blog/2023/understand-rl/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Understanding RL, especially policy gradient, could be non-trivial, particularly if you like grasping intuitions like I do. In this post, I will walk through a thread of thoughts that really helped me understand PG by starting from more familiar supervised learning setting.]]></summary></entry></feed>