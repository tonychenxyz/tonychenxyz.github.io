<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Tony Chen </title> <meta name="author" content="Haozhe (Tony) Chen "> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?2fb83a6dbc622464e2ca0f07c26f4486"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico?7d6843b300d5ece0d8adf2a3b557e43c"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://tonychenxyz.github.io/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css"> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <div class="header-content" style="display: flex; align-items: center; justify-content: space-between;"> <div class="title-and-subtitle"> <h1 class="post-title"> <span class="font-weight-bold">Haozhe (Tony) Chen</span> <br> 陈昊哲 </h1> <p class="desc"><strong>Undergraduate Student</strong>,<br> Department of Computer Science, <br> <a href="https://www.cs.columbia.edu/" rel="external nofollow noopener" target="_blank">Columbia University</a> <br> [X](https://x.com/tonychenxyz) | [Google Scholar](https://scholar.google.com/citations?user=qIy1aRQAAAAJ&amp;hl=en). <br> <a href="/assets/pdf/cv.pdf" target="_blank" title="Download CV" class="cv-icon"> <i class="fas fa-file-pdf"></i> Download CV </a> <br><br> </p> </div> <div class="profile" style="flex-shrink: 0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/profile.jpg" sizes="(min-width: 930px) 270.0px, (min-width: 1px) 30vw, 95vw"> <img src="/assets/img/profile.jpg?3d3c9c3193658afca6ace00cf10db021" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="profile.jpg" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> </div> </header> <article> <div class="clearfix"> <p><strong>Research</strong>: I am interested shaping AI ecosystem as it becomes an increasingly pervasive part of our life and society.</p> <p>Currently, I am working on:</p> <ul> <li>Efficient inference and serving of foundation models</li> <li>Human-friendly, emotion-intelligent AI</li> <li>Personalized, adaptive evaluation of foundation models</li> <li>Interpretibility of large models</li> </ul> <p>At Columbia, I have the fortunate opportunities to work with <a href="https://tianyipeng.github.io/" rel="external nofollow noopener" target="_blank">Tianyi Peng</a>, <a href="http://www.cs.columbia.edu/~mcz/" rel="external nofollow noopener" target="_blank">Chengzhi Mao</a>, <a href="https://www.cs.columbia.edu/~vondrick/" rel="external nofollow noopener" target="_blank">Carl Vondrick</a>, <a href="https://hsnamkoong.github.io/" rel="external nofollow noopener" target="_blank">Hongseok Namkoong</a>, and <a href="http://www.cs.columbia.edu/~julia/" rel="external nofollow noopener" target="_blank">Julia Hirschberg</a>.</p> <p>I am also a research intern at <a href="https://www.together.ai/" rel="external nofollow noopener" target="_blank">Together AI</a> working on accelerating inference.</p> <p><strong>Looking for Fall 2025 PhD opportunities!</strong> I am looking for Fall 2025 PhD opportunities. Please feel free to contact me via <a href="mailto:haozhe.chen@columbia.edu">haozhe.chen@columbia.edu</a>!</p> <p><strong>Previously</strong>: I was a quantitative research intern at <a href="https://www.voloridge.com/" rel="external nofollow noopener" target="_blank">Voloridge</a>, a quantitative hedge fund in Florida. I also enjoyed <a href="https://www.kaggle.com/tonychenxyz" rel="external nofollow noopener" target="_blank">Kaggle competitions</a> for a while.</p> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%74%6F%6E%79%63%68%65%6E%78%79%7A@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=qIy1aRQAAAAJ&amp;hl" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/tonychenxyz" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/haozhe-chen" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://twitter.com/tonychenxyz" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> <a href="https://www.kaggle.com/tonychenxyz" title="Kaggle" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-kaggle"></i></a> <a href="https://instagram.com/bark.bark.bark.bark.bark.bark" title="Instagram" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-instagram"></i></a> <a href="/feed.xml" title="RSS Feed"><i class="fa-solid fa-square-rss"></i></a> </div> <div class="contact-note"></div> </div> <h2> <a href="/news/" style="color: inherit">news</a> </h2> <div class="news"> <p>No news so far...</p> </div> <h2> publications </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/qgym.png" sizes="200px"> <img src="/assets/img/publication_preview/qgym.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="qgym.png" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <abbr class="badge">NeurIPS</abbr> </div> <div id="qgym" class="col-sm-8"> <div class="title">QGym: Scalable Simulation and Benchmarking of Queuing Network Controllers</div> <div class="author"> <em>Haozhe Chen </em> ,  Ang Li ,  Ethan Che ,  <a href="https://tianyipeng.github.io/" rel="external nofollow noopener" target="_blank">Tianyi Peng </a> ,  <a href="http://www.columbia.edu/~jd2736/" rel="external nofollow noopener" target="_blank">Jing Dong </a> ,  and  <a href="https://hsnamkoong.github.io/" rel="external nofollow noopener" target="_blank">Hongseok Namkoong </a> </div> <br><small>* Equal contribution</small> <div class="periodical"> <em>NeurIPS 2024 Datasets and Benchmarks</em>, Oct 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2410.06170" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/namkoong-lab/QGym" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Queuing network control determines the allocation of scarce resources to manage congestion, a fundamental problem in manufacturing, communications, and healthcare. Compared to standard RL problems, queueing problems are distinguished by unique challenges: i) a system operating in continuous time, ii) high stochasticity, and iii) long horizons over which the system can become unstable (exploding delays). To spur methodological progress tackling these challenges, we present an open-sourced queueing simulation framework, QGym, that benchmark queueing policies across realistic problem instances. Our modular framework allows the researchers to build on our initial instances, which provide a wide range of of environments including parallel servers, criss-cross, tandem, and re-entrant networks, as well as a realistically calibrated hospital queuing system. QGym makes it easy to compare multiple policies, including both model-free RL methods and classical queuing policies. Our testbed complements the traditional focus on evaluating algorithms based on mathematical guarantees in idealized settings, and significantly expands the scope of empirical benchmarking in prior work. QGym code is open-sourced at https://github.com/namkoong-lab/QGym.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/emoknob.png" sizes="200px"> <img src="/assets/img/publication_preview/emoknob.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="emoknob.png" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <abbr class="badge">EMNLP</abbr> </div> <div id="emoknob" class="col-sm-8"> <div class="title">🎛️ EmoKnob: Enhance Voice Cloning with Fine-Grained Emotion Control</div> <div class="author"> <em>Haozhe Chen </em> ,  Run Chen ,  and  <a href="https://www.cs.columbia.edu/~julia/slpg.htm" rel="external nofollow noopener" target="_blank">Julia Hirschberg </a> </div> <div class="periodical"> <em>EMNLP 2024 Main</em>, Oct 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2410.00316" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://huggingface.co/spaces/tonychenxyz/emo-knob" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Demo</a> <a href="https://github.com/tonychenxyz/emoknob" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://emoknob.cs.columbia.edu/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>While recent advances in Text-to-Speech (TTS) technology produce natural and expressive speech, they lack the option for users to select emotion and control intensity. We propose EmoKnob, a framework that allows fine-grained emotion control in speech synthesis with few-shot demonstrative samples of arbitrary emotion. Our framework leverages the expressive speaker representation space made possible by recent advances in foundation voice cloning models. Based on the few-shot capability of our emotion control framework, we propose two methods to apply emotion control on emotions described by open-ended text, enabling an intuitive interface for controlling a diverse array of nuanced emotions. To facilitate a more systematic emotional speech synthesis field, we introduce a set of evaluation metrics designed to rigorously assess the faithfulness and recognizability of emotion control frameworks. Through objective and subjective evaluations, we show that our emotion control framework effectively embeds emotions into speech and surpasses emotion expressiveness of commercial TTS services.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/hug.png" sizes="200px"> <img src="/assets/img/publication_preview/hug.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="hug.png" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <abbr class="badge">INTERSPEECH</abbr> </div> <div id="detecting-empathy" class="col-sm-8"> <div class="title">Detecting Empathy in Speech</div> <div class="author"> Run Chen ,  <em>Haozhe Chen </em> ,  Anushka Kulkarni ,  Eleanor Lin ,  Linda Pang ,  Divya Tadimeti ,  and  <a href="https://www.cs.columbia.edu/~julia/slpg.htm" rel="external nofollow noopener" target="_blank">Julia Hirschberg </a> </div> <div class="periodical"> <em>INTERSPEECH 2024</em>, Jun 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.cs.columbia.edu/speech/PaperFiles/2024/interspeech24_empathy_paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Empathy is the ability to understand another’s feelings as if we were having those feelings ourselves. It has been shown to in- crease to people’s trust and likability. Much research has been done on creating empathetic responses in text in conversational systems, yet little work has been done to identify the acoustic- prosodic speech features that can create an empathetic-sounding voice. Our contributions include 1) collection of a new empathy speech dataset, 2) identifying interpretable acoustic-prosodic features that contribute to empathy expression and 3) bench- marking the empathy detection task.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/textexplain.gif" sizes="200px"> <img src="/assets/img/publication_preview/textexplain.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="textexplain.gif" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <abbr class="badge">ICLR</abbr> </div> <div id="vit-interpret-control" class="col-sm-8"> <div class="title">INViTE: INterpret and Control Vision Transformer with Text Explanations</div> <div class="author"> <em>Haozhe Chen </em> ,  <a href="http://www.cs.columbia.edu/~junfeng/" rel="external nofollow noopener" target="_blank">Junfeng Yang </a> ,  <a href="http://www.cs.columbia.edu/~vondrick/" rel="external nofollow noopener" target="_blank">Carl Vondrick </a> ,  and  <a href="http://www.cs.columbia.edu/~mcz/" rel="external nofollow noopener" target="_blank">Chengzhi Mao </a> </div> <div class="periodical"> <em>In the 12th International Conference on Learning Representations, 2024.</em>, Jan 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2310.10591" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Large-scale pre-trained vision foundation models, such as CLIP, have become de facto backbones for various vision tasks. However, due to their black-box nature, understanding the underlying rules behind these models’ predictions and controlling model behaviors have remained open challenges. We present INViTE: a framework for INterpreting Vision Transformer’s latent tokens with Text Explanations. Given a latent token, INViTE retains its semantic information to the final layer using transformer’s local operations and retrieves the closest text for explanation. INViTE enables understanding of model visual reasoning procedure without needing additional model training or data collection. Based on the obtained interpretations, INViTE allows for model editing that controls model reasoning behaviors and improves model robustness against biases and spurious correlations.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/selfie.png" sizes="200px"> <img src="/assets/img/publication_preview/selfie.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="selfie.png" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <abbr class="badge">ICML</abbr> </div> <div id="selfie" class="col-sm-8"> <div class="title">🤳SelfIE: Self-Interpretation of Large Language Model Embeddings</div> <div class="author"> <em>Haozhe Chen </em> ,  <a href="http://www.cs.columbia.edu/~vondrick/" rel="external nofollow noopener" target="_blank">Carl Vondrick </a> ,  and  <a href="http://www.cs.columbia.edu/~mcz/" rel="external nofollow noopener" target="_blank">Chengzhi Mao </a> </div> <div class="periodical"> <em>In the 41st International Conference on Machine Learning, 2024.</em>, Mar 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2403.10949" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/tonychenxyz/selfie" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://selfie.cs.columbia.edu" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>The expanding impacts of Large Language Models (LLMs) increasingly require the answer to: How do LLMs obtain their answers? The ability to explain and control an LLM’s reasoning process is key for reliability, transparency, and future model developments. We propose SelfIE (Self-Interpretation of Embeddings) that enables LLMs to interpret their own embeddings in natural language by leveraging their ability to respond inquiry about a given passage. Capable of interpreting open-world concepts in the hidden embeddings, SelfIE reveals LLM internal reasoning in cases such as making ethical decisions, internalizing prompt injection, and recalling harmful knowledge. SelfIE’s text descriptions on hidden embeddings also open up new avenues to control LLM reasoning. We propose Supervised Control, which allows editing open-ended concepts while only requiring gradient computation of individual layer. We extend RLHF to hidden embeddings and propose Reinforcement Control that erases harmful knowledge in LLM without supervision targets. </p> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Haozhe (Tony) Chen . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"post-understand-policy-gradient-by-building-cross-entropy-from-scratch",title:'Understand Policy Gradient by Building Cross Entropy from Scratch <svg width="1.2rem" height="1.2rem" top=".5rem" viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"Understanding RL, especially policy gradient, could be non-trivial, particularly if you like grasping intuitions like I do. In this post, I will walk through a thread of thoughts that really helped me understand PG by starting from more familiar supervised learning setting.",section:"Posts",handler:()=>{window.open("https://towardsdatascience.com/understand-policy-gradient-by-building-cross-entropy-from-scratch-75ca18b53e94","_blank")}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%74%6F%6E%79%63%68%65%6E%78%79%7A@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=qIy1aRQAAAAJ&hl","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/tonychenxyz","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/haozhe-chen","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/tonychenxyz","_blank")}},{id:"socials-kaggle",title:"Kaggle",section:"Socials",handler:()=>{window.open("https://www.kaggle.com/tonychenxyz","_blank")}},{id:"socials-instagram",title:"Instagram",section:"Socials",handler:()=>{window.open("https://instagram.com/bark.bark.bark.bark.bark.bark","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>