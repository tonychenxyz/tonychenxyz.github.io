---
---
@article{vit-interpret-control,
  abbr={ICLR},
  title={INViTE: INterpret and Control Vision Transformer with Text Explanations},
  author={Chen, Haozhe and Yang, Junfeng and Vondrick, Carl and Mao, Chengzhi},
  abstract={Large-scale pre-trained vision foundation models, such as CLIP, have become de facto backbones for various vision tasks. However, due to their black-box nature, understanding the underlying rules behind these modelsâ€™ predictions and controlling model behaviors have remained open challenges. We present INViTE: a framework for INterpreting Vision Transformerâ€™s latent tokens with Text Explanations. Given a latent token, INViTE retains its semantic information to the final layer using transformerâ€™s local operations and retrieves the closest text for explanation. INViTE enables understanding of model visual reasoning procedure without needing additional model training or data collection. Based on the obtained interpretations, INViTE allows for model editing that controls model reasoning behaviors and improves model robustness against biases and spurious correlations.},
  journal={In the 12th International Conference on Learning Representations, 2024.},
  year={2024},
  month={Jan},
  dimensions={true},
  selected={true},
  arxiv={2310.10591},
  preview={textexplain.gif}
}

@article{selfie,
  abbr={Preprint},
  title={ðŸ¤³SelfIE: Self-Interpretation of Large Language Model Embeddings},
  author={Chen, Haozhe and Vondrick, Carl and Mao, Chengzhi},
  abstract={The expanding impacts of Large Language Models (LLMs) increasingly require the answer to: How do LLMs obtain their answers? The ability to explain and control an LLM's reasoning process is key for reliability, transparency, and future model developments. We propose SelfIE (Self-Interpretation of Embeddings) that enables LLMs to interpret their own embeddings in natural language by leveraging their ability to respond inquiry about a given passage. Capable of interpreting open-world concepts in the hidden embeddings, SelfIE reveals LLM internal reasoning in cases such as making ethical decisions, internalizing prompt injection, and recalling harmful knowledge. SelfIE's text descriptions on hidden embeddings also open up new avenues to control LLM reasoning. We propose Supervised Control, which allows editing open-ended concepts while only requiring gradient computation of individual layer. We extend RLHF to hidden embeddings and propose Reinforcement Control that erases harmful knowledge in LLM without supervision targets. },
  journal={Preprint.},
  year={2024},
  month={Mar},
  dimensions={true},
  selected={true},
  arxiv={2403.10949},
  preview={selfie-teaser.svg},
  website={https://selfie.cs.columbia.edu},
  code={https://github.com/tonychenxyz/selfie},
}